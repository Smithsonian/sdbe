\documentclass[11pt,preprint]{aastex}
\usepackage{natbib}
\bibliographystyle{apj}
\usepackage{amsmath,epstopdf}
\usepackage{tikz}
\usetikzlibrary{dsp,chains,shapes,arrows}
\usepackage{rotating,lscape,threeparttablex}
\DeclareGraphicsRule{.pdftex}{pdf}{*}{}

\DeclareMathAlphabet{\mathpzc}{OT1}{pzc}{m}{it}
\newcommand{\z}{\mathpzc{z}}
\newcommand{\SWARM}[1]{\texttt{#1\,SWARM}}
\newcommand{\SDBE}{\texttt{SDBE}}
\newcommand{\APHIDS}{\texttt{APHIDS}}
\newcommand{\DIFX}{\texttt{DIFX}}
\newcommand{\HASHPIPE}{\texttt{HASHPIPE}}
\newcommand{\falign}{0}
\newcommand{\talign}{7.5}

\begin{document}

\title{Resampling \SWARM{} for VLBI \\ v0.1}

\author{The APHIDS Team\altaffilmark{1}}
\altaffiltext{1}{Harvard-Smithsonian Center for Astrophysics, 60 Garden Street, Cambridge, MA 02138, USA}

\begin{abstract}
ABSTRACT
\end{abstract}

\section{Background} \label{sec:background}
\subsection{VLBI}

\subsection{\SDBE}
The \SWARM{} Digital Back End (\SDBE) is amazing.

\section{Sample Rate Conversion} \label{sec:src_basics}

Sample rate conversion, or resampling, is the process of taking a signal, $x[i]$, sampled at a rate $f_0$ and 
calculating new samples $y[i]$ at a different sample rate, $f_1$.  For \APHIDS, the ratio between the two 
sample rates is rational: $f_1/f_0 = L/M$, where $L$ and $M$ are relatively prime integers.  Operationally, 
resampling is the 
chained combination of upsampling and downsampling \citep{oppenheim10,lyons11}.  Upsampling, (also commonly 
called interpolation or expansion), 
increases the sampling rate 
by a factor of $L$ with the insertion of $L-1$ zeroes between the original $x[i]$.  A low-pass, 
anti-imaging filter smooths the signal and supresses the high frequency spectral images greater than the original 
$f_0$ that have been introduced by the zero inserts.  Figure XX sketches this process in the time and frequency 
domain for when $L$ equals 3. 

Downsampling, or decimation, by an integer factor of $M$ requires that the signal be first low-pass filtered to 
avoid aliasing 
at frequencies greater than the targeted $f_0/M$.  Only then can the sampling rate be reduced to $f_0/M$ by 
selecting every $M$th sample from the filtered signal.  Figure XX shows this process in the time and frequency 
domain for when $M$ equals 2.

Sample rate conversion by a factor $L/M$ can be conceptualized as a three step process where the original signal 
is expanded by $L$, filtered, and then decimated by $M$.  As illustrated by Figure \ref{fig:resample_basic}, 
the serial anti-imaging and 
anti-aliasing filters are combined into a single filter with cutoff frequency of 
$1/\mathrm{max}(L,M) \times Lf_0/2$.  Under this scheme, interpolation must precede decimation otherwise desired 
frequency components greater than $f_0/M$ cannot be preserved.

\begin{figure}[H!]
\centering
\label{fig:resample_basic}
\begin{tikzpicture}
   \node[dspnodeopen,dsp/label=left]  (c0) {$x[i]$};
   \node[dspsquare,right=of c0]                     (c1) {\upsamplertext{L}};
   \node[dspfilter,right=of c1]                     (c2) {$H(\z)$};
   \node[dspsquare,right=of c2]                    (c3) {\downsamplertext{M}};
   \node[dspnodeopen,right=of c3,dsp/label=right]  (c4) {$y[j]$};
   \foreach \i [evaluate = \i as \j using int(\i+1)] in {0,1,...,3}
       \draw[dspconn] (c\i) -- (c\j);
\end{tikzpicture}
\caption{Basic multirate resampling signal flow graph.}
\end{figure}

\iffalse % to save time don't compile sampling examples
\include{upsampling_tikz}
\include{downsampling_tikz}
%\include{src_tikz} % integer ratio sample rate conversion sketch
\fi %\iffalse

Computationally, the basic sample rate conversion scheme naively described in \S\ref{sec:src_basics} is highly 
inefficient.  The filtering is applied at the highest possible sample rate, $Lf_0$, on
a time-series that is $(L-1)/L$ zeros by fraction. Furthermore, the decimator discards an $(M-1)/M$ fraction of 
the samples.  As a result, this algorithm requires a lot of memory and spends precious clock 
time with wasted math.  There is a vast literature of techniques and algorithms that improve performance 
including multistage resampling, folded filter structures, and polyphase representations 
\citep{oppenheim10,lyons11,vaidyanathan93}.

There have been recent development of polyphase filter representations for GPU architectures 
\citep[i.e.][]{vanderveldt12,adamek14,kim14a}.  These filter structures can operate at the low sample rate of $f_0/M$
by introducing delays to switch the expander and decimatator \citep{crochiere81}.  However, they require 
some filter design and careful book-keeping or buffering to keep track of sample indices \citep{wang01}.
The implementation in \citet{kim14a} assigned each thread to a filter coefficient, parallelizing the 
inner product operation.  However, \citet{kim14a} found that their GPU kernel was dominated by the 
indexing operations.  \citet{adamek14} (building on the work by \citet{vanderveldt12}) presented tantalizing 
implementations of 
polyphase filter banks that could run at data rates in excess of 6.5\,GB/s (their estimate for the single channel 
output of the SKA's Low Frequency Aperture Array).

%  https://github.com/wesarmour/astro-accelerate

An operationally simple option for resampling is to use linear interpolation to estimate the values that lie in 
between the original samples:
\begin{equation}
y[i] = x[j] + (x[j+1] - x[j]) (if_0/f_1 - j)
\end{equation}
where $j = \mathrm{floor}(if_0/f_1)$.  This is equivalent to applying a ``tent'' FIR filter with $2L$ taps on the 
upsampled $Lf_0$ time series before decimation \citep{oppenhim10}.  Similarly, a $2L$ tap box-car filter is 
equivalent to nearest-neighbor interpolation:
\begin{equation}
y[i] = x[\mathrm{round}(if_0/f_1)].
\end{equation}

Neither of these methods consider the frequency content of the original signal and, as a result, perform
poorly for signals with power near Nyquist \citep{fraser89}.  To further emphasize this point, Figure 
\ref{fig:windows} shows the frequency response for linear and nearest-neighbor interpolation when 
the resampling factor $L/M$ is $64/39$.  Note that both methods have side lobes above $-40$\,dB and so the 
resampled signal will introduce a large slope in the passband (see \S XX).  However, GPUs can compute 
these low-order interpolations directly on the card using texture memory.  \citet{kim15b} utilize this feature 
by using the cuFFT library to upsample the signal in the Fourier domain by some initial factor, $U$, and then using texture memory to 
interpolate.  The resulting frequency response will follow that for a linear interpolator operating at a ...
at \citep{oppenheim10},
\begin{equation}
H_{\mathrm{lin}} = \frac{1}{L} \left[ \frac{\sin(2 \pi f L/2}{\pi f} \right]^2,
\end{equation}
multiplied by a low-pass box-car filter with cutoff frequency $f_0/2$.  It would be straight forward to 
expand upon the framework introduced by \citet{kim15b} to handle cases where $M$ is greater than $L$.  
This scheme should help reduce loss from aliasing, albeit 
at the cost of two FFTs and much larger memory requirements.

\begin{figure}[H!]
\epsscale{1.0}
\plotone{windows.eps}
\caption{Frequency response for FIR filters equivalent to linear and nearest interpolation when $L/M = 64/39$.
The orange region shows the first Nyquist zone of the target $L/Mf_0$ sample rate.  Spectral components at all 
other frequencies are aliased into this region.}
\label{fig:windows}
\end{figure}
 
\subsection{Resampling in the Fourier Domain} \label{sec:fourier_resampling}
In contrast to the previous methods, one can also implement a rational $L/M$ sample rate conversion entirely in 
the Fourier domain \citep{gold69,yeh82}.  After accumulating $kM$ samples at a clock rate of $f_0$ 
(where $k=1,2,3\cdots$), the DFT 
returns spectral components spaced at $f_0/kM$.  If $f_1 > f_0$, the resampled spectrum is generated by inserting
$p$ zeros to match the new $f_1$ while maintaining the correct frequency components: 
\begin{equation}
\frac{f_1}{kM+p} = \frac{f_0}{kM}
\end{equation} 
Solving for $p = k(L - M)$.  If $f_0 < f_1$, the spectrum is instead trimmed by $p$ samples.  The time series
sampled at $f_1$ can then be constructed from the inverse DFT.  This method is equivalent to sinc 
interpolation using an ideal low-pass filter and is a perfect interpolator
if the original, continuous signal has a discrete spectral density distribution that is band-limited below the 
Nyquist limit.  In practice, small errors may be introduced from aliased spectral leakage (see \S 
\ref{sec:short_DFT}). 

For post-processing SWARM, DFT resampling is a good fit because one can 
access arbitrary large chunks of the time series without accumulators in hardware and has well-behaved errors
that can be easily modeled.  However, the speed of the FFT depends on the resampling factors ($L/M = 32/39$ for 
\SWARM{6/11}) and zero-padding in the case that the $L\gg M$ may be costly for memory.  The relative speed of the 
FFT algorithm will also depend on the conversion factors ($L$ and $M$).

%\subsubsection{Effects from short DFTs} \label{sec:short_DFT}

It is not strictly true that DFT resampling requires no filter design. Trimming or padding de-facto multiplies 
the spectrum by a boxcar window which is equivalent to convolving a normalized sinc function with a 
\emph{periodic summation} of the original series.
Consequenty, errors will be wrapped into both edges of the 
resampled signal.  The extent of this error 
depends only on the width of the sinc function which is set by the resampling factors $L$ and $M$.  
\begin{equation} \label{eq:sinc}
B\,\mathrm{sinc}(B t) \overset{\mathcal{F}}{\Longleftrightarrow} \begin{cases} 1 \quad |f| < B/2 \\ 0 \quad |f| > B/2 \end{cases}
\end{equation}
where $B = \mathrm{min}(f_0,L/Mf_0)$.  
Therefore, the fraction of samples affected is inversely proportional to the number of samples.  Regardless, the 
number of samples for which this makes a large difference is small and can be roughly treated as less than a 
1\% effect when $N > 100\,\mathrm{max}(L/M,1)$.  One option to mitigate this error to stitch together 
overlapping, resampled segments \citep{bi11}, requiring more computation and memory.  A second strategy is to 
multiply the original signal by some window function that tapers to zero at its edges \citep{fraser89}.

Another phenomenon that becomes increasingly important for small DFTs is aliasing from the initial end-point
discontinuities.  The process of selecting $kM$ samples from the continuous signal $x(t)$ is represented as 
multiplication by a box-car window or convolution with a sinc function in the frequency domain.  The sinc window
widens for narrower box-car windows, causing spectral leakage and aliasing.  The ultimate result is also a loss 
in correlation amplitude.

\section{APHIDS} \label{sec:aphids}

We are developing a software solution to the resampling problem called \APHIDS\, (\textbf{A}daptive 
\textbf{P}hased-array and 
\textbf{H}eteregeneous \textbf{I}nterpolator and \textbf{D}ownsampler for \textbf{S}WARM).  This open-source 
code \footnote{\url{https://github.com/sma-wideband/sdbe}} is written in C/CUDA using
\HASHPIPE (\textbf{H}igh \textbf{A}vailibility \textbf{Sh}ared \textbf{Pipe}line
 Engine)\footnote{\url{https://github.com/david-macmahon/hashpipe}}.  The software is designed to use 
modularized, lightweight threads to handle reading data packets from hard drives, resampling on a GPU cluster, 
and then writing back to separate disks.  These three steps are each loosely mapped to threads we call the 
vdif\_in\_net thread, vdif\_inout\_gpu\_thread, and vdif\_out\_net thread. 
The ultimate goal of is to process data streamed directly from the 
\SDBE\, at the telescope.  The flowchart in Figure \ref{fig:aphids_flow_chart} shows how data progresses 
through the pipeline and this section will describe each block covering the major hardware components, software 
design, and data formats.  We will conclude with a short discussion of future development and applications of \APHIDS.

\tikzstyle{line} = [draw,-latex']
\tikzstyle{cpu} = [rectangle, draw, fill=blue!20,
    text width=5em, text centered, rounded corners, minimum height=4em]
\tikzstyle{gpu} = [rectangle, draw, fill=red!20,
    text width=5em, text centered, rounded corners, minimum height=4em]
\tikzstyle{resamp} = [rectangle, draw, minimum height=4.5em, align=center, minimum width=15em] 

\begin{figure}[H]
\begin{center}
\begin{tikzpicture} [node distance = 5cm, auto]
\node [cpu] (mark6_in) {Mark6 Reader};
\node[gpu,below of=mark6_in](reader){Reader};
\node[gpu,right of=reader](rIFFT){rIFFT};
\node[gpu,right of=rIFFT](resampler){Resampler};
\node[cpu,right of=resampler](VDIF_pkt){VDIF packet \\ \& header};
\node[cpu,below of=VDIF_pkt](mark6_out){Mark6 Writer};

\path[line] (mark6_in) -- node[align=center] {BENG @ 2496 GHz \\ quantized to 2-bits}(reader);
\path[line] (reader) -- node[align=center] {spectra of \\ 16k complex64} (rIFFT);
\path[line] (rIFFT) --  node[align=center] {time series of \\ 32k float32} (resampler);
\path[line] (resampler) --  node[align=center] {time series \\ @ 2048 GHz}(VDIF_pkt);
\path[line] (VDIF_pkt) -- node[align=center, left]{VDIF @ 4096 GHz \\ quantized to 2-bits}(mark6_out);
\end{tikzpicture}
\label{fig:aphids_flow_chart}
\caption{This signal flow chart shows an overview of \APHIDS.  The red boxes represent operations on the GPU while blue boxes are for the CPU.}
\end{center}
\end{figure}

\subsection{Hardware description}

\APHIDS\, freatures three main components of hardware: an I/O data system, CPU, and GPU server.  For I/O, the 
EHT observations taken by \SWARM\, in March, 2015 have been stored on four disk-packs that each hold eight, 
6\,TB hard drives (totaling $48 \times 4 = 192$\,TB of data).  The Mark6 VLBI data system has four modules that can hold one disk-pack and we use
two seprate Mark6 units to handle the recording and playback of data.  \APHIDS\, itself is run on a separate CPU 
that controls the GPU server.  Our machine has two 6-core Intel Xeon 2.4 Ghz 
processors which limits the number of concurrent threads to 12.

The GPU server as four GTX 980 GPUs, each with 4GB of local memory.  Data from the CPU (or host) is copied over 
a PCI-E bus into the local GPU (or device) memory.  Once the data is transferred, the host directs the device
to perform functions (or kernels) on the data which must be again moved from the local device memory onto 
on-chip memory for the individual streaming multiprocessors that make up the GPU.  After computation is done, 
the host transfers the data off of the device memory.

\subsection{Data I/O and formats}

The observations were recorded using a scatter/gather file system which disperses the 
data composing individual scans over the four modules.  \APHIDS\, uses fast, general purpose software called 
sgcomm (\textbf{s}catter \textbf{g}ather \textbf{comm}unication) to read in 
the scattered data into CPU shared memory over a 10 Gigabit Ethernet network.  In order to keep the \HASHPIPE\, 
threads sufficiently lightweight, each input thread handles XX bytes which are accumulated into an XX byte 
buffer by the inout thread.  Similarly, each output thread is fed only XX bytes of the total XX bytes of the 
resamped data and are then streamed back over 10 Gigabit Ethernet to a separate Mark6 for recording.

Data from the \SDBE\, follows a customized B-engine format where each
frame is composed of 1024, 1056B VDIF frames \footnote{\url{http://www.vlbi.org/vdif/}}.  128 snapshots of a 
16384 sample \SWARM{}\, spectrum makes up one B-engine frame. However, the current \SDBE\, bitcode packs
neither the snapshots contiguously in time nor the spectrum samples contiguously in frequency.  Furthermore, 
each B-frames overlaps the preceding one in memory.  Our resampling algorithm handles $39n$\,\SWARM{}\, snapshots
simultaneously, so in order to reduce the amount of book-keeping the inout thread has 4 buffer XX bytes, 
each of which holds 39 complete B-engine frames.  The inout thread does preliminary checks to ensure that the 
B-engine frames are complete before assigning each buffer to a GPU.



The data is written as a time series using VDIF format.  More to come on that.

\subsection{Resampling Block}\label{sec:resamp_block}

\APHIDS\, resamples the data in the Fourier domain (see \S \ref{sec:fourier_resampling}).  There were several 
contributing factors that led us to make
this choice rather than apply filters in the time domain.  First, the GPU cluster can handle relatively large 
chunks of data at a time so there losses from the DFT are very manageable.  Second, regardless of 
what resampling algorithm is used the \SWARM\, spectra must 
be transformed into the time domain, the guard bands trimmed and the signal modulated.  These operations are done 
very simply using FFTs and in the Fourier domain.  However, the cost of the
resampling operation will be dominated by these operations.  Lastly, we can utilize the optimized cuFFT library
\footnote{\url{https://developer.nvidia.com/cuFFT}} to ensure good performace, reduce development time and 
maintain a manageably sized codebase.

The \APHIDS\, resampling block is composed of three 1-dimensional FFTs (A, B, and C; see Figure \ref{fig:resampling_block}).  
Following the cuFFT API reference to improve performace, we utilize batched
FFTs (launching more than one FFT at a time using the cuFFT API), out-of-place transforms, 
and ensure that transforms operate on contiguous chunks of memory.  We also use single precisiong transforms to 
reduce the memory transfer bandwidth and ensure that 
the resampling block can be run on a single device.  The latter point is important since it allows the inout
thread to parallelize the resampling operation across the cluster and provide a x4 speedup.

The first FFT (A) takes the complex valued, 16385 sample \SWARM\, 
spectrum into the time domain, returning 
32768 real samples.  Since \SWARM\, omits the Nyquist sample which is far outside the \SWARM\, cutoff frequency, 
we insert a zero as the 16385th sample.  Since this transform is radix-2, the performance should benefit from 
the Cooley-Tukey algorithm.  However, on our cluster the cuFFT library exhibits linear improvement in Gflops 
with transform size up to N of 16384 (see Figure \ref{fig:C2R_performance}).  This behavior is consistent with the CUDA 7.0 performance 
report\footnote{\url{http://on-demand.gputechconf.com/gtc/2015/webinar/gtc-express-cuda7-performance-overview.pdf}} 
and we find that this transform, just ouside of the optimal regime, operates at 6.7 Gsps.

Next, we Fourier transform chunks of $N_B = 19968$ consecutive time series samples (B).  The resulting 
spectrum has a frequency resolution of $2496$\,MHz$/19958=125$\,kHz.  The next step is to apply a third
complex-to-real transform (C) on a subset of the spectrum, using only frequences from $150$\,MHz to $1174$\,MHz.  
The final time series has size $N_C= 2\times(1174-150)/0.125=16384$ per batch element and our desired 
sampling rate of 
$16384 \times 0.125$\,MHz$ = 2048$\,MHz.  This simple operation simultaneously resamples the signal, modulates 
the signal, and trims the guard bands.  Furthermore, it is easily implemented using cuFFT by using the same 
batch size as the second real-to-complex transform, increasing the input pointer index by $150 / 0.125 = 1200$
(equivalent to masking out the first 1200 channels) and then using a input stride of $9980$ for the 
$N=16384$ complex-to-real transform.

The choice of FFT size was set to maximize performance.  In order for the target sampling rate to be achieved
$N_B$ must be a multiple of 39.  Furthermore, so that the 150\,MHz guard band can be removed, we must ensure that 
150 is a multiple of $2496/N_B$:
\begin{equation}
\frac{150\times39}{2496} = \frac{75}{32} = \frac{i}{j}, \quad i,j = 1, 2,3, \ldots
\end{equation}
To ensure this, $j$ should be a multiple of 32 so that 
\begin{equation}
N_B = 39j = 1248i, \quad i=1,2,3,\ldots
\end{equation}
Lastly, the size of the C transform is then determined:
\begin{equation}
N_C = \frac{32}{39} N_B.
\end{equation}
The dashed line in Figure \ref{fig:C2R_performance} shows the performance for candidate $N_B$s.  Unlike for 
the radix-2 cases, the configuration that has the best Gflop performance does not also provide the best Gsps
rate.  This is likely because the Gflop calculation is not an actual flop count, but a scaling from the 
original Cooley-Tukey algorithm \citep{cooley59}.  The Gsps metric is perhaps more useful in this case as it
shows that choices of $N_B <= 19968$ will have good performance.  However, the figure also clearly demonstrates
that this non radix-2 FFT is about $2\times$ slower than the radix-2 case.


\begin{figure}[H!]
\epsscale{0.95}
\plotone{C2R_performance.eps}
\caption{This figure shows the performance of radix-2, single precision, batched, complex-to-real, 1D transforms 
measured on hamster. The benchmark conventions follow FFTW  (http://www.fftw.org/speed/method.html).}
\label{fig:C2R_performance}
\end{figure}

\subsection{Timing?}

The Mark6 can record data to the four modules at 16 Gbps.

%% Loss table : /home/krosenfe/resampling/losses.py
% losses for FIR filtering, interpolation schemes, and FFT
\begin{deluxetable}{l|ccc}
\tablecolumns{4}
\tablewidth{0pc}
\tablecaption{Correlation coefficient loss from resampling \label{tab:loss}}
\tablehead{\colhead{Method} & \colhead{64/39} & \colhead{32/39} & \colhead{128/125}}
\startdata
Nearest-Neighbor            & 13\% & 17\% & 13\% \\
Linear                      &  5\% &  7\% &  5\% \\
Hamming window ($16L$ taps) &  1\% &  2\% &  2\% \\
FFT ($N=M$)                 &  1\% &  1\% & 0.2\%
\enddata
\end{deluxetable}

\subsection{FFT Accuracy}
One concern with using multiple FFTs to change the sampling rate is the error introduced by bit growth.
The floating point error grows as $\mathcal{O}\sqrt{\log{N}}$ \citep{schatzman96} on average for the Cooley-Tukey
algorithm, but errors can be exacerbated by innacurate twiddle factors or choice of FFT algorithm (CITE
benchFFT?).  We have computed the accuracy of the cufft library using single precision for 
radix-2 transforms as well as the $N_b = 19968$ case in Figure \ref{fig:fft_accuracy}.  These values 
were computed using benchFFT\footnote{\url{http://www.fftw.org/accuracy/}} and we compare against 
fftw3\footnote{\url{http://www.fftw.org/}}
results also computed on hamster.  For random input, the cufft library follows the expected $\mathcal{O}\sqrt{\log{N}}$ behavior
but with larger scatter and slightly worse performance than fftw3.

\begin{figure}[H!]
\plotone{fft_accuracy.eps}
\caption{Accuracy of the FFT algorithm as measured by the $L_2$ error from benchFFT} \label{fig:fft_accuracy}
%/home/krosenfe/testing/cufft_accuracy/make_figure.py
\end{figure}

Round off error and bit growth is a general concern for scientific computing on GPUs.  
Increasing the precision to double can significantly slow down performance and then 
require reoptimization or even the redesign of kernels.  Generally, well implemented FFTs can 
be very stable options.  The results of ours test suggest that 
bit growth will not be a large source of loss for the resampling scheme presented in \S\ref{sec:resampling_block}.

\subsection{Quantization}
Another source of loss and distortion to consider is the requantization back to 2-bits that is done
before writing back to disk on the Mark6 (see Figure \ref{fig:aphids_flow_chart}).  In theory, \APHIDS\,could 
record higher bit data (or even the full 32 bits for floating point), but this adds complications for 
correlation 
in \DIFX\ and would incur costs for both disk space and output data rate.  While the former cost is monetary, the 
latter could be an important limiting factor for real-time operation of \APHIDS\,at the back end of the \SDBE\,
(see \S XX).  The signal-to-noise loss expected from 2-bit quantization is a function of the quantization 
thresholds, but for Gaussian noise and Nyquist sampling is at best 12\% \citep{cooper70,thompson01}.

\subsection{Future Development}

Include some discussion on how quantization choice could limit the data rate for APHIDS. 
Mention interest in recording 4-bit data to deal with SWARM unbalanced state counts.
%Use GTX 980 specs: http://www.geforce.com/hardware/desktop-gpus/geforce-gtx-980/specifications

Include some discussion on operation of hamster at the summit.  Will cooling be an issue?


\begin{figure}
\centering
\begin{tikzpicture}[node distance = 2.2cm, auto]
\node[align=center](init){};
\node[resamp,below of=init](rfft){batched real FFT \\ dimension of $39 \times N$};
\node[resamp,below of=rfft](trim){Trim band and shift frequency};
\node[resamp,below of=trim](rifft){batched real IFFT \\ dimension of $32 \times N$};
\node[align=center,below of=rifft](final){};

\path[line](init) -- node[align=left]{ time series \\ @ 2496\,MHz} (rfft);
\path[line](rfft) -- (trim);
\path[line](trim) -- (rifft);
\path[line](rifft) -- node[align=left]{ time series \\ @ 2048\,MHz}(final);
\end{tikzpicture}
\label{fig:resampling_block}
\caption{The resampling block trims the SWARM guard bands and shifts the passband to DC as part of the 
resampling operation.}
\end{figure}

\begin{figure}[H]
\epsscale{1.0}
\plotone{SWARM_amp_spectra.eps}
% /home/krosenfe/sdbe/software/prototyping/cuda/SWARM_amp_spectra.eps
\caption{This figure shows both channels of the detrended amplitude spectral density of SWARM along with the 
R2DBE (SMA single-dish in gray).}
\label{fig:swarm_amp_spec}
\end{figure}

\section{Results}

Include AY initial correlation tests with single dish and reported SWARM fringes with LMT.
We should reproduce these results using \APHIDS\ (or as much of it as possible).

\section{The Team}
The text to this document was written by KR with referral to intermediate documents written by
members of the \APHIDS\ team.  LB ran simulations on 2-bit quantization and suggested the DFT resampling 
architecture appropriate for \SWARM{6/11}. AY wrote sgcomm, the front and back-end kernels de-and re-packetizing 
the data along with quantization. RP handled \HASHPIPE\, and the pipeline design.  The B-engine data format along 
with the \SDBE\, bit code was developed by LV and RP.

\acknowledgments 
\clearpage

\bibliography{sdbe}
\end{document}
